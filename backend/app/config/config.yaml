llm:
  provider: "ollama"
  default_model: "llama3.2"
  
  # Provider-specific configurations
  providers:
    openai:
      api_url: "https://api.openai.com/v1/chat/completions"  # Optional: override default
      models:
        gpt-4o:
          rpm: 5000.0
          tpm: 10000000.0
        gpt-4o-mini:
          rpm: 15000.0
          tpm: 10000000.0
        gpt-4-turbo:
          rpm: 5000.0
          tpm: 10000000.0
        gpt-4:
          rpm: 500.0
          tpm: 30000.0
        gpt-3.5-turbo:
          rpm: 10000.0
          tpm: 1000000.0
    
    ollama:
      api_url: "http://localhost:11434/api/chat"  # Optional: override default
      models:
        llama3.2:
          rpm: 100.0
          tpm: 100000.0